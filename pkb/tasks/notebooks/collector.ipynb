{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler, EdgeDataLoader\n",
    "from dgl.dataloading.negative_sampler import Uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dgl import save_graphs, load_graphs\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, HeteroGraphConv\n",
    "from dgl.utils import expand_as_pair\n",
    "from collections import defaultdict\n",
    "import torch as th\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data.utils import makedirs, save_info, load_info\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "import time\n",
    "import re\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089156be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '~/Downloads/harvard-index-collectors.csv'  ## macOS file location, harvard index collector\n",
    "harvard_data = pd.read_csv(file1,chunksize=10000,encoding='utf-8',on_bad_lines='skip',engine='python')\n",
    "harvard_data = pd.concat(harvard_data)\n",
    "harvard_columns = harvard_data.columns.tolist()\n",
    "harvard_data.rename(columns={harvard_columns[8]:'labelName'}, inplace=True) ## to avoid special character in colNames\n",
    "\n",
    "harvard_columns = harvard_data.columns.tolist()   ## load content into list\n",
    "harvard_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = '~/Downloads/wikidata-collectors-230615.csv'  ## macOS file location, wikipedia collector\n",
    "\n",
    "wiki_data = pd.read_csv(file2,chunksize=10000,encoding='utf-8',on_bad_lines='skip',engine='python')\n",
    "wiki_data = pd.concat(wiki_data)\n",
    "wiki_columns = wiki_data.columns.tolist()   ## load content into list\n",
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83515de",
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_data['combine_specialty'] = combine_specialty(harvard_data)\n",
    "harvard_data['combine_geography'] = combine_geography(harvard_data)\n",
    "wiki_data['dateOfBirthYear'] = convert_date2year(wiki_data,'dateOfBirth')\n",
    "wiki_data['dateOfBirthYear'] = convert_date2year(wiki_data,'dateOfBirth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d94527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute and plot the data distribution of a given dataframe\n",
    "'''\n",
    "Example:\n",
    "distribution = get_distribution(wiki_data, wiki_columns)\n",
    "temp = distribution.T.sort_values(by=['sum'], ascending=False)\n",
    "print(temp)\n",
    "temp.plot.bar(figsize=(15,10), title = 'WikiData')\n",
    "'''\n",
    "def get_distribution(data, col):\n",
    "    sum_count = 0\n",
    "    data_distribution = {} ## a dictionary to store the distribution of individual entity\n",
    "    data_distribution_sum = pd.DataFrame(columns = col, index=['sum'])\n",
    "    for (columnName, columnData) in data.iteritems():\n",
    "        temp = data[columnName].value_counts()\n",
    "        data_distribution_sum.at['sum', columnName]=sum(temp)\n",
    "    return data_distribution_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485279d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# this function will remove all special charaters -- including spaces\n",
    "# used process_time() for evaluation\n",
    "Example:\n",
    "newCol = remove_spec_in_col(wiki_data,'aliases')\n",
    "'''\n",
    "# Remove square blankets auto generated during data alignment process\n",
    "def clean_text(text): # fb\n",
    "    text = text.replace('[', '').replace(']','').replace(\"'\", '')\n",
    "    return text\n",
    "\n",
    "def remove_spec_in_col(df, col):\n",
    "    newCol = []\n",
    "    for index, rowValue in df[col].iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol.append(clean_text(rowValue))\n",
    "        else:\n",
    "            newCol.append(np.nan)\n",
    "    return newCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Harvard Index data preprocessing\n",
    "Example:\n",
    "text1 = \"[author note: Types at: MT] [collector note: col. with M. St-Arnaud; MT; BO]\"\n",
    "text2 = \"[collector note: Herbarium (Piedmont (Italy): TO]\"\n",
    "text3 = \"[collector note: SAFB] plant pathologist; short biography and photo: Can. J. Plant Pathol. Vol. 28: S21-S22. 2006.\"\n",
    "text4 = \"[collector note: BM-SL, OXF.]\"\n",
    "text5 = \"[collector note: Herbarium and types, US, additional m,erial , B, C, DBN, GH, K, MIN, MO, NA, NY, PH\"\n",
    "text = \"[author note: LE; temperate Asian Polygonaceae] [collector note: LE] \"\n",
    "temp = text4.split(\"[\")\n",
    "\n",
    "s1 = get_author_notes(temp)\n",
    "s2 = get_collector_notes(temp)\n",
    "print(\"\\nFunction version:\")\n",
    "print(\"author note: \", s1)\n",
    "for i in s1: print(\"\\nHerbarium List: \", get_herbarium_codes(i))\n",
    "print(\"collector note: \", s2)\n",
    "for i in s2: print(\"\\nHerbarium List: \", get_herbarium_codes(i))\n",
    "\n",
    "a, b = get_author_collector_notes(harvard_data, 'Remarks')\n",
    "'''\n",
    "\n",
    "# Functions to extract the herbarium institution codes from Remarks in Harvard Index\n",
    "def get_herbarium_codes(string):\n",
    "    herbarium_codes = []\n",
    "    for s in string.split(\",\"):\n",
    "        if s.isupper():\n",
    "            herbarium_codes.append(re.sub('[^A-Z]', ',', s).replace(\",\",\"\"))\n",
    "    return herbarium_codes\n",
    "            \n",
    "def get_author_notes(string):\n",
    "    authorNotes = []\n",
    "    for s in string:\n",
    "        # s = clean_text(s)\n",
    "        # authorNotes.append(s.partition(\"author note: \")[2].partition(\" \")[0].replace(\";\", ''))\n",
    "        authorNotes.append(s.partition(\"author note: \")[2].partition(\"]\")[0].replace(\";\", ',').replace(\":\", ',').replace(\"(\",\",\").replace(\")\",\",\").replace(\"at\",\",\"))\n",
    "    authorNotes = list(filter(None, authorNotes))\n",
    "    return authorNotes\n",
    "\n",
    "def get_collector_notes(string):\n",
    "    collectorNotes = []\n",
    "    for s in string:\n",
    "        # s = clean_text(s)\n",
    "        # collectorNotes.append(s.partition(\"collector note: \")[2].partition(\" \")[0].replace(\";\", ''))\n",
    "        collectorNotes.append(s.partition(\"collector note: \")[2].partition(\"]\")[0].replace(\";\", ',').replace(\":\", ',').replace(\"(\",\",\").replace(\")\",\",\").replace(\"at\",\",\"))\n",
    "    collectorNotes = list(filter(None, collectorNotes))\n",
    "    return collectorNotes\n",
    "\n",
    "def get_author_collector_notes(df, col):\n",
    "    authorNoteCol = []\n",
    "    collectorNoteCol = []\n",
    "    for index, rowValue in df[col].iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            string = rowValue.split(\"[\")\n",
    "            authorNoteCol.append(get_author_notes(string))\n",
    "            collectorNoteCol.append(get_collector_notes(string))\n",
    "        else:\n",
    "            authorNoteCol.append(np.nan)\n",
    "            collectorNoteCol.append(np.nan)\n",
    "    return authorNoteCol, collectorNoteCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a424868",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Return the cleaned year value of Wikidata\n",
    "Example:\n",
    "new_df['dateOfBirthYear'] = convert_date2year(wiki_data,'dateOfBirth')\n",
    "'''\n",
    "def get_year(date_str):\n",
    "    # Remove + sign\n",
    "    if date_str[0] == '+':\n",
    "        date_str = date_str[1:]\n",
    "    return int(date_str[0:4])\n",
    "\n",
    "def convert_date2year(df, col):\n",
    "    newCol = []\n",
    "    for index, rowValue in df[col].iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol.append(get_year(rowValue))\n",
    "        else:\n",
    "            newCol.append(np.NaN)\n",
    "    return newCol\n",
    "\n",
    "# Not used in the data cleaning process\n",
    "def get_timestamp(date_str):\n",
    "    # Probably not necessary\n",
    "    date_str = date_str.strip()\n",
    "    # Remove + sign\n",
    "    if date_str[0] == '+':\n",
    "        date_str = date_str[1:]\n",
    "    # Remove missing month/day\n",
    "    date_str = date_str.split('-00', maxsplit=1)[0]\n",
    "    # Parse date\n",
    "    dt = np.datetime64(date_str)\n",
    "    # As Unix timestamp (choose preferred datetype)\n",
    "    return dt.astype('<M8[s]').astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Harvard Index data preprocessing\n",
    "Example:\n",
    "temp = harvard_data[['id','Name','Specialty Author', 'Specialty Collector', 'Specialty Determiner', 'Specialty']].copy()\n",
    "temp['combine_specialty'] = combine_specialty(temp)\n",
    "# temp['combine_specialty'].value_counts().index.tolist()\n",
    "'''\n",
    "# Extract specialty areas of collectors\n",
    "def combine_specialty(df):\n",
    "    cols = ['Specialty Author', 'Specialty Collector', 'Specialty Determiner', 'Specialty']\n",
    "    newCol = df[cols].apply(lambda row: ','.join(row.dropna().unique()), axis=1)\n",
    "    for index, rowValue in newCol.iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol[index] = set(clean_text(rowValue).replace('And',',').replace(' and ',',').replace(' ','').split(','))\n",
    "        else:\n",
    "            newCol[index] = np.nan\n",
    "    return newCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Harvard Index data preprocessing\n",
    "Example:\n",
    "temp = harvard_data[['id','Name','Geography Author', 'Geography Collector', 'Geography Determiner', 'Geography']].copy()\n",
    "temp['combine_geography'] = combine_geography(temp)\n",
    "# temp['combine_geography'].value_counts().index.tolist()\n",
    "'''\n",
    "# Extract geography locations and travel history related to collectors\n",
    "def combine_geography(df):\n",
    "    cols = ['Geography Author', 'Geography Collector', 'Geography Determiner', 'Geography']\n",
    "    newCol = df[cols].apply(lambda row: ','.join(row.dropna().unique()), axis=1)\n",
    "    for index, rowValue in newCol.iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol[index] = set(clean_text(rowValue).replace('\\xa0 ','').split(','))\n",
    "        else:\n",
    "            newCol[index] = np.nan\n",
    "    return newCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Combining Wikidata and Harvard Index data\n",
    "Example:\n",
    "wiki_data['harvardIndex'] = return_numeric(wiki_data, 'harvardIndex')\n",
    "\n",
    "# But it's the same as the below inbuilt function\n",
    "wiki_data['harvardIndex'] = pd.to_numeric(wiki_data['harvardIndex'],errors='coerce') ## wrap wiki id to int64\n",
    "'''\n",
    "# Helper function to extract numerical numbers from a data column\n",
    "def return_numeric(df, col):\n",
    "    newCol = []\n",
    "    for index, rowValue in df[col].iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol.append(re.sub(\"[^0-9|.]\", \"\", str(rowValue)))\n",
    "        else:\n",
    "            newCol.append(rowValue)\n",
    "    return newCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef0f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
